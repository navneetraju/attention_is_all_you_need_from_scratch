{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4b4d6b",
   "metadata": {},
   "source": [
    "# Transformer From Scratch – Module Tests\n",
    "\n",
    "This notebook walks through unit‑testing each custom module in the repository and finally runs a tiny end‑to‑end forward pass with the full `TransformerEncoderDecoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc611b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:07.729182Z",
     "start_time": "2025-04-11T17:09:06.926746Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from modules.blocks.decoder_block import TransformerDecoderBlock\n",
    "from modules.blocks.encoder_block import TransformerEncoderBlock\n",
    "from modules.core.embeddings import TokenEmbedding, PositionalEmbedding\n",
    "from modules.core.multi_head_attention import MultiHeadedAttention\n",
    "from modules.core.scaled_dot_product_attention import ScaledDotProductAttention\n",
    "from modules.transformer import TransformerEncoderDecoder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/git/personal/attention_is_all_you_need_from_scratch/.venv/lib/python3.9/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3a4649ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:07.751597Z",
     "start_time": "2025-04-11T17:09:07.740142Z"
    }
   },
   "source": [
    "# ---- Token & Positional Embeddings ----\n",
    "batch, seq_len, d_model = 2, 5, 32\n",
    "vocab_size = 100\n",
    "tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "pos_emb = PositionalEmbedding(max_seq_length=50, embedding_dim=d_model)\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch, seq_len))\n",
    "tok = tok_emb(x)\n",
    "pos = pos_emb(seq_len).unsqueeze(0)\n",
    "print('Token embedding:', tok.shape)\n",
    "print('Positional embedding:', pos.shape)\n",
    "print('Sum:', (tok + pos).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding: torch.Size([2, 5, 32])\n",
      "Positional embedding: torch.Size([1, 5, 32])\n",
      "Sum: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4b5f4823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:07.821308Z",
     "start_time": "2025-04-11T17:09:07.817040Z"
    }
   },
   "source": [
    "# ---- Scaled Dot‑Product Attention ----\n",
    "attn = ScaledDotProductAttention(embedding_dim=d_model)\n",
    "out = attn(tok)  # (B, L, d)\n",
    "print('Scaled attention output:', out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5170353f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:07.912645Z",
     "start_time": "2025-04-11T17:09:07.907551Z"
    }
   },
   "source": [
    "# ---- Multi‑Head Attention ----\n",
    "heads = 4\n",
    "mha = MultiHeadedAttention(embedding_dim=d_model, num_heads=heads)\n",
    "out_mha = mha(tok, tok, tok)\n",
    "print('Multi‑head output:', out_mha.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi‑head output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "48316f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:07.964453Z",
     "start_time": "2025-04-11T17:09:07.958770Z"
    }
   },
   "source": [
    "# ---- Encoder Block ----\n",
    "ff_dim = 64\n",
    "enc = TransformerEncoderBlock(d_model, heads, ff_dim)\n",
    "enc_out = enc(tok + pos)\n",
    "print('Encoder block output:', enc_out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "212bb946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:08.048231Z",
     "start_time": "2025-04-11T17:09:08.042147Z"
    }
   },
   "source": [
    "# ---- Decoder Block ----\n",
    "dec = TransformerDecoderBlock(d_model, heads, ff_dim)\n",
    "dec_in = tok + pos  # pretend previous target embeddings\n",
    "dec_out = dec(dec_in, enc_out, enc_out)\n",
    "print('Decoder block output:', dec_out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder block output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b6540149",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:09:08.223463Z",
     "start_time": "2025-04-11T17:09:08.204304Z"
    }
   },
   "source": [
    "# ---- Full Transformer Encoder‑Decoder ----\n",
    "model = TransformerEncoderDecoder(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    max_seq_length=50,\n",
    "    embedding_dim=d_model,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    num_heads=heads,\n",
    "    feed_forward_dim=ff_dim\n",
    ")\n",
    "\n",
    "src = torch.randint(0, vocab_size, (batch, seq_len))\n",
    "tgt = torch.randint(0, vocab_size, (batch, seq_len))\n",
    "logits = model(src, tgt)\n",
    "print('Transformer output logits:', logits.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output logits: torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e9e732e42b96f19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
